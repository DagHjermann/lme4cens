---
title: "Censored Response Examples"
author: "MK"
date: "2017-02-27, `r Sys.Date()`"
output:
  rmarkdown::html_vignette:
    toc: true
    fig_caption: yes
params:
  
vignette: >
  %\VignetteIndexEntry{Censored Response Examples}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include=FALSE, message=FALSE}
library(dplyr)

devtools::dev_mode(on = TRUE)
## knitr package options
knitr::opts_knit$set(verbose = FALSE)
options(digits = 3L)

library(survival)

library(lme4cens)
library(microbenchmark)

library(censReg)
library(crch)

data("Affairs", package = "lme4cens")

VERBOSE <- 0L
```

We demonstrate examples for regression analysis for censored observations.


## Censored observations in the `survival`-package
`Surv`-objects are used to hold a response value and its status information.
You provide a `time` and an `event` column. If `event` is missing (and it is *not* type `interval2`) `Surv` assumes that all subjects have an event.
It is safest to use logical coding for the `event`-column as the software does not have to guess if 0/1 or 1/2 was used.

A `type`-parameter lets you specify which type of censoring is used. It hence guides the meaning of the censoring:

* `right` censoring means right-censoring (e.g. the unobserved value is greater (right) than `time`. It is default type when there is no `time2` and `event` is _not_ a factor. `event` information is used to know which times are real observations, and which are right-censoring times.
* `left` censoring means left-censoring (e.g. the unobserved value is smaller (left) than `time`).
* `interval` to encode for potentially different kind of censorings in one dataset.
    * `event=0` indicates right-censoring,
    * `event=1` indicates observed event,
    * `event=2` indicates left-censoring.
    * `event=3` for interval-censoring. Only then it uses the `time2` information.
* `interval2` for interval censorings. All observations are considered as interval-censored observations and `event`-column is not used but instead, `time2` is always used. To mark infinity, you can use `Inf` or `NA`. `type='interval2'` is easier to code for, internally the stored type of `interval2` is still `interval`.
    * (-Inf, t) for left-censored
    * (t,t) for exact observations
    * (t, Inf) for right-censored
    * (t~1~, t~2~) for interval-censored

`Surv`-objects are implemented as a matrix.
For instance, four observations with no censoring, left-, right- and interval censoring as represented as a matrix.
```{r Surv_obj_mat, echo = FALSE}
survObj <- Surv(time = c(3, -Inf, 3, 3), time2 = c(3, 3, Inf, 5), type = "interval2")
survObj
as.matrix(survObj)
```




## Exponential distribution
We generate random data (both, event and censoring independent of each other) from a Weibull distribution.

```{r expLeftCens_data}
set.seed(123L)
N <- 230L
λ <- 1.8

ex_exp_lCens <- data_frame_(list(
  X = ~ rweibull(n = N, shape = 1L, scale = 1/λ),
  Cl = ~ rweibull(n = N, shape = 1.2, scale = 1/λ-.3), # shorter mean time than for response X
  T = ~ pmax(X, Cl),
  status = ~ as.numeric(X >= Cl)
))
```


The left-censoring does not allow for an easy analytic solution. Hence, we do an own numeric likelihood maximization.
We use a factory function `negLogLik` that --given a data sample-- produces a negative likelihood function.
`optimize` is R's standard function for 1-dimensional optimization.

```{r expLeftCens_logLik}

# factory method for negative log-likelihood function
# function log1mexp efficiently calculates log(1-exp(x))
negLogLik <- function(data){
  stopifnot( is.data.frame(data), all(c("T", "status") %in% names(data)) )
  
  nbrEvents <- sum(data$status)
  eventIdx <- data$status == 1L
  
  function(λ)
    - nbrEvents * log(λ) + λ * sum(data$T[eventIdx]) - sum(lme4cens::log1mexp(-λ * data$T[!eventIdx]))
}


optimize(negLogLik(data = ex_exp_lCens),
         interval = c(0, 10))
```

The same result can be achieved via `survival::survreg`:

```{r expLeftCens_survreg}
fm.exp <- survreg(Surv(T, status, type = "left") ~ 1, dist = "exp",
        data = ex_exp_lCens)
fm.exp
```


The linear predictor η is transformed to our λ via 1/exp(η),
i.e. λ = 1/ exp(`r coef(fm.exp)`) = `r exp(-coef(fm.exp))`.







## Linear Regression with censored observations: Tobit-model

Tobit model uses normal error. In order to compare our fits with fits from other packages we use a fixed censoring value for all observations.
Here we use the `Affairs`-data as an example with left-censored response at 0.

We have prepared a column `event` that encodes for the censoring at 0:
```{r affairs_event_boxplot}
boxplot(affairs ~ event, data = Affairs, xlab = "Variable 'event'", ylab = "Number of affairs")
```


### Simple regression
We start with a single predictor `yearsmarried`. We use default Nelder-Mead and BFGS with gradient.

```{r affairs_simple_lmcens}
fm.aff.simple <- lmcens(Surv(affairs, event, type = "left") ~ yearsmarried, data = Affairs)
summary(fm.aff.simple)

fm.aff.simple.bfgs <- lmcens(Surv(affairs, event, type = "left") ~ yearsmarried,
                             method = "BFGS",
                             data = Affairs)
summary(fm.aff.simple.bfgs)
```

We compare the output with the fit from `censReg`.
```{r affairs_simple_censReg, echo = FALSE}
fm.simple.aff.censreg <- censReg(affairs ~ yearsmarried, left = 0L, data = Affairs)

summary(fm.simple.aff.censreg)
```

The results of both model fitting routines do agree.





### Multiple regression

We use Nelder-Mead optimization by default.
```{r affairs_multiple_lmcens}
fm.aff <- lmcens(Surv(affairs, event, type = "left") ~ age + yearsmarried + religiousness + occupation + rating,
                 data = Affairs)

summary(fm.aff)
```


Now, we use `BFGS` as optimization method.

```{r affairs_multiple_lmcens_bfgs}
fm.aff.bfgs <- lmcens(Surv(affairs, event, type = "left") ~ age + yearsmarried + religiousness + occupation + rating,
                       ##start = c(coef(lm(affairs ~ age + yearsmarried + religiousness + occupation + rating, data = Affairs)), 2),
                      method = "BFGS",
                      data = Affairs)

summary(fm.aff.bfgs)
```

#### Effect of bad start values

When we use the BFGS-optimization method with rough start values (all zeros) we get no convergence at a good model:

```{r affairs_multiple_lmcens_bfgs_badStart}
fm.aff.bfgs.badstart <- lmcens(Surv(affairs, event, type = "left") ~ age + yearsmarried + religiousness + occupation + rating,
                      start = rep(0, 7L),
                      method = "BFGS",
                      data = Affairs)

summary(fm.aff.bfgs.badstart)
```

The fit is erroneous!!
We get very different coefficients to the previous fit and the log-likelihood is clearly worse.
So, good start values do matter for bigger models (with possibly correlated predictors).




#### Multiple Tobit model with `censReg`

We compare our results with the output of `censReg`.

```{r affairs_multiple_censReg__NelderMead}
fm.aff.censreg <- censReg(affairs ~ age + yearsmarried + religiousness + occupation + rating, 
                          left = 0L,
                          data = Affairs)

summary(fm.aff.censreg)
```

The coefficients are very close to those of our `BFGS`-fit!!




We look into the log-likelihood contributions per observations between the two fits when we fit with coefficients 
as estimated by Nelder-Mead at `lmcens`.


```{r ex1_compare_logLiks}
llContribs <- attr(fm.aff$logLik.contribs, "loglik.contribs")
llContribs.censreg <- update(fm.aff.censreg, start = coef(fm.aff), logLikOnly = TRUE)

MASS::eqscplot(x = llContribs, y = llContribs.censreg); abline(0,1, col = "grey", lty = 2)
```

The log-likelihood for the optimal fit from `lmcens` is calculated by `censReg` as `r sum(llContribs.censreg)`.


When we start our `lmcens` with the parameter estimates of `censReg` (rounded on 1 digit) using Nelder-Mead we stay in that optimum region and get very similar results:
```{r ex1_lmcens_affairs_censRegStart}
fm.aff2 <- lmcens(Surv(affairs, event, type = "left") ~ age + yearsmarried + religiousness + occupation + rating,
                  start = round(coef(fm.aff.censreg),3),
                  data = Affairs)

summary(fm.aff2)
```

We plot the log-likelihood contributions from our fitted `lmcens`-model with start parameters coming from `censReg`-fit and also calculate these contributions via the `censReg`-package.

```{r ex1_compare_logLiks2}
llContribs2 <- attr(fm.aff2$logLik.contribs, "loglik.contribs")
llContribs2.censreg <- update(fm.aff.censreg, start = coef(fm.aff2), logLikOnly = TRUE)

MASS::eqscplot(x = llContribs2, y = llContribs2.censreg); abline(0,1, col = "grey", lty = 2)
```



### Tobit Model with `crch`

```{r ex1_crch_affairs}

fm.crch.aff <- crch(affairs ~ age + yearsmarried + religiousness + occupation + rating,
                    data = Affairs, left = 0, dist = "gaussian")
summary(fm.crch.aff)
```

The package `crch` also coincides with the default fit from `censReg`.



### With weights

We explicitly give higher weight to cases with positive religiousness and positive number of affairs.
The resulting coefficient for `religiousness` changes accordingly.


```{r affairs_lmcens_w}
wPos <- with(Affairs, which(affairs > 1 & religiousness > 2))
w <- rep(1L, NROW(Affairs))
w[wPos] <- 2.5


fm.aff.w.nm <- lmcens(Surv(affairs, event, type = "left") ~ age + yearsmarried + religiousness + occupation + rating,
              weights = w, data = Affairs)
summary(fm.aff.w.nm)
```


We check the results from `BFGS`-fitting:
```{r affairs_lmcens_w_bfgs}


fm.aff.w.bfgs <- lmcens(Surv(affairs, event, type = "left") ~ age + yearsmarried + religiousness + occupation + rating,
                        method = "BFGS",
                        weights = w, data = Affairs)
summary(fm.aff.w.bfgs)
```





## Mixed Linear Models with censored observations

We use a censored version of the `sleepstudy`-data, by applying the information in column `event3` for left- and right-censored measurements of reaction time.
We use fixed boundaries (column `event3`) for left- and right-censoring so that we can compare the results with `censReg`.

```{r ex2_sleepstudy2_lme4cens}
REACT_L <- 212
REACT_R <- 350

sleepstudy2 %>% 
  lFormula(Surv(pmin(pmax(Reaction, REACT_L), REACT_R), time2 = Reaction, event = event3,
                type = "interval") ~ Days + (1|Subject), data = ., REML=FALSE) -> 
  lForm



lForm %>% 
  append(list(verbose=VERBOSE, quadrature = "stats")) %>%
  do.call(mkLmerCensDevfun_rInt_R, .) ->
  
  myDevFun_f

# Gauss-Hermite quadrature
lForm %>% 
  append(list(verbose=VERBOSE, quadrature = "gh")) %>%
  do.call(mkLmerCensDevfun_rInt_R, .) ->
  
  myDevFun_gh_f


## optimize function
paramS <- c(260, 5, 3, 2)

paramEst <- optim(par = paramS, fn = myDevFun_gh_f)
paramEst
```



### Comparison with `censReg`

We compare it with the results of `censReg` on the same data.

```{r ex2_sleepstudy2_censReg, message = FALSE}

sleepstudy2 %>% 
  dplyr::mutate_(Reaction = ~ pmin(pmax(Reaction, REACT_L), REACT_R))  %>% 
  plm::pdata.frame(index = c("Subject", "Days")) ->
  
  sleepstudy2.cr

fm.censReg <- censReg::censReg(Reaction ~ as.numeric(Days), left = REACT_L, right = REACT_R,
                               start = paramS, nGHQ = 8L, data = sleepstudy2.cr)

summary(fm.censReg)
```

The fitted coefficients are different, in particular the intercept coefficient ${\hat β}_0 =$ `r paramEst$par[1]` (`lme4cens`) vs `r coef(fm.censReg)[1]` (`censReg`). The mean value of the observations per time point are:
```{r sleepstudy2_desc, echo = FALSE, results='asis'}
sleepstudy2 %>%
  dplyr::group_by_(~ Days) %>%
  dplyr::summarise_(mReaction = ~ mean(Reaction)) %>% 
  knitr::kable(digits = 0L)
```




We use our method with start values that are close to the optimal values of the `censReg`-fit:
```{r ex2_sleepstudy2_lme4censLL}
optim(par = coef(fm.censReg), fn = myDevFun_gh_f)
```

The variance components change in comparison to the given initial optimal `censReg` values.

```{r ex2_sleepstudy2_censRegLL, echo = FALSE}
ll.censReg <- censReg::censReg(Reaction ~ as.numeric(Days), left = REACT_L, right = REACT_R, start = paramEst$par, logLikOnly = TRUE, nGHQ = 8L,
                               data = sleepstudy2.cr)

nll.lme4cens <- myDevFun_gh_f(param = paramEst$par)
nll.lme4cens.numIntStats <- myDevFun_f(param = paramEst$par)
```

Vice versa, we check the log-likelihood at the _optimal fit_ of our own implementation.
`censReg` calculates the log-likelihood contributions per subject, together with its gradient.
The log-likelihood contributions are `r paste(round(ll.censReg, 2), collapse = ", ")` which sum to `r sum(ll.censReg)`.
Our implementation using GH-quadrature at the same start vector gives a log-likelihood of `r -nll.lme4cens` with subject-contributions `r paste(round(log(attr(nll.lme4cens, "lik.contribs")), 2), collapse = ", ")`.
When using numeric integration from `stats`-package we get :
`r -nll.lme4cens.numIntStats` with subject-contributions `r paste(round(log(attr(nll.lme4cens.numIntStats, "lik.contribs")), 2), collapse = ", ")`




