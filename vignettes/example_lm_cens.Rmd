---
title: "Linear Models with Censoring Examples"
author: "MK"
date: "2017-02-27, `r Sys.Date()`"
output:
  rmarkdown::html_vignette:
    toc: true
    fig_caption: yes
vignette: >
  %\VignetteIndexEntry{Linear Models with Censoring Examples}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include=FALSE, message=FALSE}
library(dplyr)

devtools::dev_mode(on = TRUE)
## knitr package options
knitr::opts_knit$set(verbose = FALSE)
options(digits = 3L)

library(survival)

library(lme4cens)
library(microbenchmark)

library(censReg)
library(crch)

data("Affairs", package = "lme4cens")

VERBOSE <- 0L
```

We demonstrate examples for regression analysis for censored observations.



## Exponential distribution
We generate random data (both, event and censoring independent of each other) from a Weibull distribution.

```{r expLeftCens_data}
set.seed(123L)
N <- 230L
λ <- 1.8

ex_exp_lCens <- data_frame_(list(
  X = ~ rweibull(n = N, shape = 1L, scale = 1/λ),
  Cl = ~ rweibull(n = N, shape = 1.2, scale = 1/λ-.3), # shorter mean time than for response X
  T = ~ pmax(X, Cl),
  status = ~ as.numeric(X >= Cl)
))
```


The left-censoring does not allow for an easy analytic solution. Hence, we do an own numeric likelihood maximization.
We use a factory function `negLogLik` that --given a data sample-- produces a negative likelihood function.
`optimize` is R's standard function for 1-dimensional optimization.

```{r expLeftCens_logLik}

# factory method for negative log-likelihood function
# function log1mexp efficiently calculates log(1-exp(x))
negLogLik <- function(data){
  stopifnot( is.data.frame(data), all(c("T", "status") %in% names(data)) )
  
  nbrEvents <- sum(data$status)
  eventIdx <- data$status == 1L
  
  function(λ)
    - nbrEvents * log(λ) + λ * sum(data$T[eventIdx]) - sum(lme4cens::log1mexp(-λ * data$T[!eventIdx]))
}


optimize(negLogLik(data = ex_exp_lCens),
         interval = c(0, 10))
```

The same result can be achieved via `survival::survreg`:

```{r expLeftCens_survreg}
fm.exp <- survreg(Surv(T, status, type = "left") ~ 1, dist = "exp",
        data = ex_exp_lCens)
fm.exp
```


The linear predictor η is transformed to our λ via 1/exp(η),
i.e. λ = 1/ exp(`r coef(fm.exp)`) = `r exp(-coef(fm.exp))`.







## Linear Regression with censored observations: Tobit-model

Tobit model uses normal error. In order to compare our fits with fits from other packages we use a fixed censoring value for all observations.
Here we use the `Affairs`-data as an example with left-censored response at 0.

We have prepared a column `event` that encodes for the censoring at 0:
```{r affairs_event_boxplot}
boxplot(affairs ~ event, data = Affairs, xlab = "Variable 'event'", ylab = "Number of affairs")
```


### Simple regression
We start with a single predictor `yearsmarried`. We use default Nelder-Mead and BFGS with gradient.

```{r affairs_simple_lmcens}
fm.aff.simple <- lmcens(Surv(affairs, event, type = "left") ~ yearsmarried, data = Affairs)
summary(fm.aff.simple)

fm.aff.simple.bfgs <- lmcens(Surv(affairs, event, type = "left") ~ yearsmarried,
                             method = "BFGS",
                             data = Affairs)
summary(fm.aff.simple.bfgs)
```

#### censReg
We compare the output with the fit from `censReg`.
```{r affairs_simple_censReg, echo = FALSE}
fm.aff.simple.censreg <- censReg(affairs ~ yearsmarried, left = 0L, data = Affairs)

summary(fm.aff.simple.censreg)
```

The results of both model fitting routines do agree.


#### `survreg`
Parametric regression is supported in `survival::survreg` function, also known as accelerated failure time (AFT) models.
It allows for different error distributions. We choose `'gaussian'` to specify the same model:

```{r affairs_simple_survreg, echo = FALSE}
fm.aff.simple.censreg <- survreg(Surv(affairs, event, type = "left") ~ yearsmarried, dist = "gaussian", data = Affairs)
summary(fm.aff.simple.censreg)
```





### Multiple regression


We use Nelder-Mead (NM) optimization by default.  
```{r affairs_multiple_lmcens}
fm.aff <- lmcens(Surv(affairs, event, type = "left") ~ age + yearsmarried + religiousness + occupation + rating,
                 data = Affairs)

summary(fm.aff)
```

NM does not use the gradient information and therefore easily gets stuck in a local maximum of the log-likelihood funciton.

Now, we use `BFGS` as optimization method.

```{r affairs_multiple_lmcens_bfgs}
fm.aff.bfgs <- lmcens(Surv(affairs, event, type = "left") ~ age + yearsmarried + religiousness + occupation + rating,
                       ##start = c(coef(lm(affairs ~ age + yearsmarried + religiousness + occupation + rating, data = Affairs)), 2),
                      method = "BFGS",
                      data = Affairs)

summary(fm.aff.bfgs)
```


#### Effect of bad start values

When we use the BFGS-optimization method with rough start values (all zeros) we get no convergence at a good model:

```{r affairs_multiple_lmcens_bfgs_badStart}
fm.aff.bfgs.badstart <- lmcens(Surv(affairs, event, type = "left") ~ age + yearsmarried + religiousness + occupation + rating,
                      start = rep(0, 7L),
                      method = "BFGS",
                      data = Affairs)

summary(fm.aff.bfgs.badstart)
```

The fit is erroneous!!
We get very different coefficients to the previous fit and the log-likelihood is clearly worse.
So, good start values do matter for bigger models (with possibly correlated predictors).




#### Multiple Tobit model with `censReg`

We compare our results with the output of `censReg`.

```{r affairs_multiple_censReg__NelderMead}
fm.aff.censreg <- censReg(affairs ~ age + yearsmarried + religiousness + occupation + rating, 
                          left = 0L,
                          data = Affairs)

summary(fm.aff.censreg)
```

The coefficients are very close to those of our `BFGS`-fit!!


#### Double checks

##### Likelihood contributions
We look into the log-likelihood contributions per observations of the NM-fit, once in our implementation and the `censReg` implementation.
The contriubtions really coincide.

```{r ex1_compare_logLiks}
llContribs <- attr(fm.aff$logLik.contribs, "loglik.contribs")
llContribs.censreg <- update(fm.aff.censreg, start = coef(fm.aff), logLikOnly = TRUE)

MASS::eqscplot(x = llContribs, y = llContribs.censreg); abline(0,1, col = "grey", lty = 2)
```

The log-likelihood for the optimal fit from `lmcens` is calculated by `censReg` as `r sum(llContribs.censreg)`.


##### NM with good start values
When we start our `lmcens` with the parameter estimates of `censReg` (rounded on 1 digit) using Nelder-Mead we stay in that optimum region and get very similar results:
```{r ex1_lmcens_affairs_censRegStart}
fm.aff2 <- lmcens(Surv(affairs, event, type = "left") ~ age + yearsmarried + religiousness + occupation + rating,
                  start = round(coef(fm.aff.censreg),3),
                  data = Affairs)

summary(fm.aff2)
```

##### Likelihood contributions at `censReg`-start values
We plot the log-likelihood contributions from our fitted `lmcens`-model with start parameters coming from `censReg`-fit and also calculate these contributions via the `censReg`-package.

```{r ex1_compare_logLiks2}
llContribs2 <- attr(fm.aff2$logLik.contribs, "loglik.contribs")
llContribs2.censreg <- update(fm.aff.censreg, start = coef(fm.aff2), logLikOnly = TRUE)

MASS::eqscplot(x = llContribs2, y = llContribs2.censreg); abline(0,1, col = "grey", lty = 2)
```



#### Tobit Model with `crch`

```{r ex1_crch_affairs}

fm.aff.crch <- crch(affairs ~ age + yearsmarried + religiousness + occupation + rating,
                    data = Affairs, left = 0, dist = "gaussian")
summary(fm.aff.crch)
```

The package `crch` also coincides with the default fit from `censReg`.



#### `survreg`
We use parametric AFT-models from `survival`-package. Specifying a Gaussian error distribution within `survreg` should yield the same model.

```{r affairs_survreg, echo = FALSE}
fm.aff.censreg <- survreg(Surv(affairs, event, type = "left") ~ age + yearsmarried + religiousness + occupation + rating, dist = "gaussian", data = Affairs)
summary(fm.aff.censreg)
```





### Multiple regression with weights

We explicitly give higher weight to cases with positive religiousness _and_ positive number of affairs.
The resulting coefficient for `religiousness` changes from significantly negative towards 0 accordingly.


#### lmcens with NM
```{r affairs_lmcens_w}
wPos <- with(Affairs, which(affairs > 1 & religiousness > 2))
w <- rep(1L, NROW(Affairs))
w[wPos] <- 2.5


fm.aff.w.nm <- lmcens(Surv(affairs, event, type = "left") ~ age + yearsmarried + religiousness + occupation + rating,
              weights = w, data = Affairs)
summary(fm.aff.w.nm)
```


#### lmcens with BFGS
We check the results from `BFGS`-fitting:
```{r affairs_lmcens_w_bfgs}

fm.aff.w.bfgs <- lmcens(Surv(affairs, event, type = "left") ~ age + yearsmarried + religiousness + occupation + rating,
                        method = "BFGS",
                        weights = w, data = Affairs)
summary(fm.aff.w.bfgs)
```



#### survreg
```{r affairs_survreg_w, echo = FALSE}
fm.aff.w.survreg <- survreg(Surv(affairs, event, type = "left") ~ age + yearsmarried + religiousness + occupation + rating,
                        dist = "gaussian",
                        weights = w, data = Affairs)
summary(fm.aff.w.survreg)
```

The estimates for coefficients are virtually the same with our `BFGS`-fit.
But standard errors and log-likelihood value differ between the two implementations.

